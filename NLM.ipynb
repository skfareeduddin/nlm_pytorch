{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcff9eac",
   "metadata": {},
   "source": [
    "# Assignment 2: Neural Language Model Training (PyTorch)\n",
    "\n",
    "**Applicant:** Syed Khaja Fareeduddin  \n",
    "**Project:** Neural Language Model Training (PyTorch) — Assignment 2  \n",
    "**Date:** November 2025  \n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "The goal of this assignment is to **train a neural language model from scratch** using PyTorch,  \n",
    "demonstrating understanding of:\n",
    "- Sequence modeling (RNN/LSTM/Transformer)\n",
    "- Model generalization and overfitting control\n",
    "- Perplexity evaluation\n",
    "\n",
    "The dataset used is *Pride and Prejudice* by Jane Austen, which is used here for character-level modeling.\n",
    "\n",
    "We will:\n",
    "1. Implement a character-level **LSTM language model**  \n",
    "2. Show **underfitting**, **overfitting**, and **best-fit** scenarios  \n",
    "3. Evaluate models using **perplexity**  \n",
    "4. Implement a small **Transformer-based model** for **extra credit**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f052f273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from src.data_preprocessing import get_data_loaders\n",
    "from src.model_lstm import LSTMLanguageModel\n",
    "from src.model_transformer import TransformerLanguageModel\n",
    "from src.train import train_model\n",
    "from src.evaluate import calculate_perplexity\n",
    "from src.utils import setup_device, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "device = setup_device()\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "os.makedirs(\"outputs/models\", exist_ok=True)\n",
    "os.makedirs(\"outputs/plots\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77139f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded successfully!\n",
      "Vocabulary Size: 61\n",
      "Training Batches: 5001 | Validation Batches: 555\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"dataset\\Pride_and_Prejudice-Jane_Austen.txt\"\n",
    "\n",
    "train_loader, val_loader, vocab_size, char2idx, idx2char = get_data_loaders(\n",
    "    dataset_path, seq_length=100, batch_size=128, split_ratio=0.9\n",
    ")\n",
    "\n",
    "print(f\"✅ Data loaded successfully!\")\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Training Batches: {len(train_loader)} | Validation Batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79b974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMLanguageModel(\n",
      "  (embedding): Embedding(61, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc): Linear(in_features=512, out_features=61, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LSTMLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=256,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c3c31",
   "metadata": {},
   "source": [
    "### Training Configurations\n",
    "\n",
    "We will train the LSTM model under three different setups to demonstrate model capacity and generalization.\n",
    "\n",
    "| Scenario | Description | Model Capacity | Epochs | Learning Rate | Notes |\n",
    "|-----------|--------------|----------------|---------|----------------|--------|\n",
    "| **Underfit** | Too small model or insufficient training | Low | 2 | 0.01 | Model fails to learn patterns |\n",
    "| **Overfit** | Large model trained too long | High | 10 | 0.001 | Model memorizes training data |\n",
    "| **Best Fit** | Balanced setup | Medium | 10 | 0.001 | Good generalization |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ffaa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda3\\envs\\nlm\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Train Loss: 1.2364, Val Loss: 10.9278\n",
      "Epoch [2/2] - Train Loss: 1.1489, Val Loss: 11.1735\n",
      "✅ Model saved at outputs/models/lstm_underfit.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.2363974537474707, 1.1489114054821177],\n",
       " [10.927833142151703, 11.17349977751036])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "underfit_model = LSTMLanguageModel(\n",
    "    vocab_size, embedding_dim=64, hidden_dim=128, num_layers=1, dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "train_model(\n",
    "    underfit_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    vocab_size,\n",
    "    device,\n",
    "    num_epochs=2,\n",
    "    lr=0.01,\n",
    "    save_path=\"outputs/models/lstm_underfit.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f609ec26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.7872, Val Loss: 14.8647\n",
      "Epoch [2/10] - Train Loss: 0.2436, Val Loss: 17.5146\n",
      "Epoch [3/10] - Train Loss: 0.2034, Val Loss: 18.2254\n",
      "Epoch [4/10] - Train Loss: 0.1899, Val Loss: 19.0806\n",
      "Epoch [5/10] - Train Loss: 0.1826, Val Loss: 18.9426\n",
      "Epoch [6/10] - Train Loss: 0.1780, Val Loss: 19.4334\n",
      "Epoch [7/10] - Train Loss: 0.1749, Val Loss: 19.6974\n",
      "Epoch [8/10] - Train Loss: 0.1725, Val Loss: 19.7313\n",
      "Epoch [9/10] - Train Loss: 0.1709, Val Loss: 19.3954\n",
      "Epoch [10/10] - Train Loss: 0.1695, Val Loss: 19.5496\n",
      "✅ Model saved at outputs/models/lstm_overfit.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7871992266063713,\n",
       "  0.24361783116787725,\n",
       "  0.2034203131582422,\n",
       "  0.18994292164237422,\n",
       "  0.1825759515532778,\n",
       "  0.17797398864269448,\n",
       "  0.17485796712775442,\n",
       "  0.17252679697169562,\n",
       "  0.1708572585418019,\n",
       "  0.1695114145503476],\n",
       " [14.864702106166531,\n",
       "  17.514624542373795,\n",
       "  18.22541817845525,\n",
       "  19.08060347582843,\n",
       "  18.942589828559946,\n",
       "  19.43341186111038,\n",
       "  19.69739401533797,\n",
       "  19.731322647644593,\n",
       "  19.395444735965214,\n",
       "  19.54960112528758])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overfit_model = LSTMLanguageModel(\n",
    "    vocab_size, embedding_dim=384, hidden_dim=768, num_layers=3, dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "train_model(\n",
    "    overfit_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    vocab_size,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    lr=0.001,\n",
    "    save_path=\"outputs/models/lstm_overfit.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2eb5916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.9318, Val Loss: 12.3119\n",
      "Epoch [2/10] - Train Loss: 0.5135, Val Loss: 15.2905\n",
      "Epoch [3/10] - Train Loss: 0.4163, Val Loss: 16.3688\n",
      "Epoch [4/10] - Train Loss: 0.3750, Val Loss: 16.9875\n",
      "Epoch [5/10] - Train Loss: 0.3506, Val Loss: 17.4820\n",
      "Epoch [6/10] - Train Loss: 0.3338, Val Loss: 18.0732\n",
      "Epoch [7/10] - Train Loss: 0.3213, Val Loss: 18.2724\n",
      "Epoch [8/10] - Train Loss: 0.3113, Val Loss: 18.1644\n",
      "Epoch [9/10] - Train Loss: 0.3034, Val Loss: 18.8153\n",
      "Epoch [10/10] - Train Loss: 0.2967, Val Loss: 19.0747\n",
      "✅ Model saved at outputs/models/lstm_bestfit.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.9318091202034899,\n",
       "  0.5135033684262179,\n",
       "  0.41631231352558756,\n",
       "  0.3750272436252572,\n",
       "  0.350563401569011,\n",
       "  0.33377648483298106,\n",
       "  0.32129098201150824,\n",
       "  0.3113477679937512,\n",
       "  0.3034050701928363,\n",
       "  0.2966784136077448],\n",
       " [12.311916408023318,\n",
       "  15.290514650430765,\n",
       "  16.368750042958304,\n",
       "  16.98749641212257,\n",
       "  17.48203630361471,\n",
       "  18.073157750378858,\n",
       "  18.27239241900745,\n",
       "  18.164423643576132,\n",
       "  18.815329158198725,\n",
       "  19.07470783371109])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestfit_model = LSTMLanguageModel(\n",
    "    vocab_size, embedding_dim=256, hidden_dim=512, num_layers=2, dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "train_model(\n",
    "    bestfit_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    vocab_size,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    lr=0.001,\n",
    "    save_path=\"outputs/models/lstm_bestfit.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9248bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Underfit model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda3\\envs\\nlm\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n",
      "C:\\Users\\skfar\\AppData\\Local\\Temp\\ipykernel_2380\\1417174212.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Underfit Model Perplexity: 71217.92\n",
      "\n",
      "Evaluating Overfit model...\n",
      "Overfit Model Perplexity: 309231617.51\n",
      "\n",
      "Evaluating Best Fit model...\n",
      "Best Fit Model Perplexity: 192327043.73\n"
     ]
    }
   ],
   "source": [
    "configs = [\n",
    "    (\"Underfit\", \"outputs/models/lstm_underfit.pth\", dict(embedding_dim=64, hidden_dim=128, num_layers=1, dropout=0.1)),\n",
    "    (\"Overfit\", \"outputs/models/lstm_overfit.pth\", dict(embedding_dim=384, hidden_dim=768, num_layers=3, dropout=0.1)),\n",
    "    (\"Best Fit\", \"outputs/models/lstm_bestfit.pth\", dict(embedding_dim=256, hidden_dim=512, num_layers=2, dropout=0.3)),\n",
    "]\n",
    "\n",
    "for name, path, cfg in configs:\n",
    "    print(f\"\\nEvaluating {name} model...\")\n",
    "    model = LSTMLanguageModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=cfg[\"embedding_dim\"],\n",
    "        hidden_dim=cfg[\"hidden_dim\"],\n",
    "        num_layers=cfg[\"num_layers\"],\n",
    "        dropout=cfg[\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    ppl = calculate_perplexity(model, val_loader, vocab_size, device)\n",
    "    print(f\"{name} Model Perplexity: {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765bf07",
   "metadata": {},
   "source": [
    "## Extra Credit: Transformer Language Model\n",
    "\n",
    "As part of the extra credit work, we implement a lightweight Transformer-based character level language model.  \n",
    "The Transformer uses self-attention mechanisms to capture long-range dependencies in text, which traditional LSTMs struggle with.\n",
    "\n",
    "### Why Transformer?\n",
    "Unlike LSTMs that process input sequentially, Transformers process the entire sequence in parallel,  \n",
    "making them efficient and effective at modeling contextual relationships, especially in longer sentences or documents.\n",
    "\n",
    "### Model Configuration\n",
    "- **Embedding Dimension:** 256  \n",
    "- **Number of Heads:** 4  \n",
    "- **Number of Layers:** 2  \n",
    "- **Feedforward Network:** 512 hidden units  \n",
    "- **Optimizer:** Adam (lr = 0.001)  \n",
    "- **Epochs:** 8  \n",
    "\n",
    "We'll train it similarly to the LSTM models and compare validation perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb3c6fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda3\\envs\\nlm\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8] - Train Loss: 0.5178, Val Loss: 0.3266\n",
      "Epoch [2/8] - Train Loss: 0.0211, Val Loss: 0.2174\n",
      "Epoch [3/8] - Train Loss: 0.0191, Val Loss: 0.2105\n",
      "Epoch [4/8] - Train Loss: 0.0183, Val Loss: 0.1565\n",
      "Epoch [5/8] - Train Loss: 0.0179, Val Loss: 0.1900\n",
      "Epoch [6/8] - Train Loss: 0.0173, Val Loss: 0.0936\n",
      "Epoch [7/8] - Train Loss: 0.0170, Val Loss: 0.0781\n",
      "Epoch [8/8] - Train Loss: 0.0167, Val Loss: 0.0701\n",
      "✅ Model saved at outputs/models/transformer.pth\n",
      "Transformer Model Perplexity: 1.07\n"
     ]
    }
   ],
   "source": [
    "transformer_model = TransformerLanguageModel(\n",
    "    vocab_size, embed_dim=256, num_heads=4, num_layers=2\n",
    ").to(device)\n",
    "\n",
    "train_model(\n",
    "    transformer_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    vocab_size,\n",
    "    device,\n",
    "    num_epochs=8,\n",
    "    lr=0.001,\n",
    "    save_path=\"outputs/models/transformer.pth\"\n",
    ")\n",
    "\n",
    "transformer_perplexity = calculate_perplexity(transformer_model, val_loader, vocab_size, device)\n",
    "print(f\"Transformer Model Perplexity: {transformer_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aefa90",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "\n",
    "| Model | Training Epochs | Parameters | Validation Perplexity | Observation |\n",
    "|--------|-----------------|-------------|------------------------|-------------|\n",
    "| **LSTM (Underfit)** | 2 | Small | ~71,000 | Strong underfitting, model failed to learn meaningful structure. |\n",
    "| **LSTM (Overfit)** | 10 | Large | ~309,000,000 | Severe overfitting, model memorized training data and generalization collapsed. |\n",
    "| **LSTM (Best Fit)** | 10 | Medium | ~192,000,000 | Better training stability but still poor generalization, validation loss increased. |\n",
    "| **Transformer (Extra Credit)** | 8 | Medium | 1.07 | Excellent generalization, captured long-range dependencies and outperformed all LSTMs. |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The **underfitting LSTM** was too small to capture the text structure, resulting in extremely high perplexity.  \n",
    "- The **overfitting LSTM** memorized the training data but failed to generalize, leading to exploding validation perplexity.  \n",
    "- The **best-fit LSTM** improved training loss but still showed poor validation performance, indicating unstable learning.  \n",
    "- The **Transformer model** achieved very low validation loss (0.0701) and perplexity ≈ 1.07, demonstrating:\n",
    "  - Strong modeling of long-range dependencies  \n",
    "  - Stable and efficient training  \n",
    "  - Dramatic improvement compared to all LSTM variants  \n",
    "\n",
    "The Transformer clearly provided the best performance among all tested models.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs\n",
    "\n",
    "- All loss plots are saved in: `outputs/plots/`\n",
    "- All trained model files are located in: `outputs/models/`\n",
    "- The accompanying PDF report summarizes the methodology, experiments, and findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
